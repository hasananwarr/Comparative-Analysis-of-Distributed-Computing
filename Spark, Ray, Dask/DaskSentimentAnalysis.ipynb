{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PY4z62KujxGZ"
      },
      "outputs": [],
      "source": [
        "!pip install \"bokeh>=3.1.0\" dask dask[complete] dask-ml scikit-learn nltk lightgbm pyngrok --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import dask.dataframe as dd\n",
        "import dask\n",
        "from dask_ml.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import pandas as pd\n",
        "from dask_ml.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from pyngrok import ngrok\n",
        "import time\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from lightgbm.dask import DaskLGBMClassifier\n",
        "from dask.distributed import Client, LocalCluster\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('punkt_tab')\n",
        "stop_words = set(stopwords.words(\"english\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EJ7djwokRtN",
        "outputId": "4dbfb970-a9c6-4d9e-8962-c421f689479d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def download_and_extract(dataset, file_name):\n",
        "    download_path = \"./downloads\"\n",
        "    zip_file_path = file_name + \".zip\"\n",
        "    command = f\"kaggle datasets download {dataset} --file {file_name}\"\n",
        "    os.system(command)\n",
        "\n",
        "    os.system(f\"unzip {zip_file_path} -d {download_path}\")\n",
        "    extracted_file_path = os.path.join(download_path, file_name)\n",
        "\n",
        "    return os.path.join(download_path, file_name)\n",
        "\n",
        "def clean_and_label(df_chunk):\n",
        "    df_chunk[\"review_body\"] = (\n",
        "        df_chunk[\"review_body\"]\n",
        "        .fillna(\"\")\n",
        "        .str.lower()\n",
        "        .str.replace(r\"http\\S+|www\\S+|https\\S+\", \"\", regex=True)\n",
        "        .str.replace(r\"[^a-zA-Z\\s]\", \"\", regex=True)\n",
        "        .str.replace(r\"\\s+\", \" \", regex=True)\n",
        "        .str.strip()\n",
        "    )\n",
        "    df_chunk[\"star_rating\"] = pd.to_numeric(df_chunk[\"star_rating\"], errors=\"coerce\")\n",
        "    df_chunk[\"sentiment\"] = df_chunk[\"star_rating\"].apply(lambda x: 1 if x > 3 else 0)\n",
        "    return df_chunk\n",
        "\n",
        "def tokenize_and_filter(df_chunk):\n",
        "    df_chunk[\"tokens\"] = df_chunk[\"review_body\"].fillna(\"\").apply(\n",
        "        lambda text: [\n",
        "            word for word in word_tokenize(text)\n",
        "            if word.isalpha() and word not in stop_words and len(word) > 2\n",
        "        ]\n",
        "    )\n",
        "    return df_chunk\n",
        "\n",
        "def apply_tfidf(df_chunk, num_features):\n",
        "    df_chunk[\"joined_tokens\"] = df_chunk[\"tokens\"].apply(lambda x: \" \".join(x))\n",
        "\n",
        "    vectorizer = TfidfVectorizer(max_features=num_features)\n",
        "    tfidf_matrix = vectorizer.fit_transform(df_chunk[\"joined_tokens\"])\n",
        "\n",
        "    features = tfidf_matrix.toarray()\n",
        "    feature_df = pd.DataFrame(features, columns=[f\"feature_{i}\" for i in range(features.shape[1])])\n",
        "    feature_df[\"sentiment\"] = df_chunk[\"sentiment\"].values\n",
        "\n",
        "    return feature_df"
      ],
      "metadata": {
        "id": "SDALSrs2kuyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!ngrok authtoken xxxxxxxxxxxx\n",
        "\n",
        "client = Client(n_workers=4, threads_per_worker=2)\n",
        "print(client)\n",
        "\n",
        "\n",
        "#public_url = ngrok.connect(8787)\n",
        "#print(f\"Dask Dashboard: {public_url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TqgavmNkxwQ",
        "outputId": "7afbfddc-c8aa-41bd-e541-810cf321ed39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:distributed.http.proxy:To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\n",
            "INFO:distributed.scheduler:State start\n",
            "INFO:distributed.diskutils:Found stale lock file and directory '/tmp/dask-scratch-space/scheduler-dwmqpkgy', purging\n",
            "INFO:distributed.scheduler:  Scheduler at:     tcp://127.0.0.1:39503\n",
            "INFO:distributed.scheduler:  dashboard at:  http://127.0.0.1:8787/status\n",
            "INFO:distributed.scheduler:Registering Worker plugin shuffle\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:43547'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:45849'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:34283'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:46647'\n",
            "INFO:distributed.scheduler:Register worker addr: tcp://127.0.0.1:46401 name: 0\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:46401\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:39768\n",
            "INFO:distributed.scheduler:Register worker addr: tcp://127.0.0.1:41009 name: 3\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:41009\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:39754\n",
            "INFO:distributed.scheduler:Register worker addr: tcp://127.0.0.1:33835 name: 2\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:33835\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:39774\n",
            "INFO:distributed.scheduler:Register worker addr: tcp://127.0.0.1:42205 name: 1\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:42205\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:39782\n",
            "INFO:distributed.scheduler:Receive client connection: Client-19067df9-f936-11ef-817e-0242ac1c000c\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:39796\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Client: 'tcp://127.0.0.1:39503' processes=4 threads=8, memory=334.56 GiB>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = \"cynthiarempel/amazon-us-customer-reviews-dataset\"\n",
        "file_name = \"amazon_reviews_us_Wireless_v1_00.tsv\"\n",
        "\n",
        "file_path = download_and_extract(dataset, file_name)\n",
        "\n",
        "ddf = dd.read_csv(\n",
        "    file_path,\n",
        "    sep=\"\\t\",\n",
        "    dtype={\"star_rating\": \"float32\"},\n",
        "    on_bad_lines=\"skip\",\n",
        "    engine=\"python\",\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "gM_PuRMmk8Mw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ddf = ddf.sample(frac=0.5, random_state=42)"
      ],
      "metadata": {
        "id": "jdL4-E7xrMQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(ddf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOKsv03BloQ3",
        "outputId": "7be06221-f9d5-48ec-a530-599802520776"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "898795"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ddf = ddf.repartition(npartitions=4)"
      ],
      "metadata": {
        "id": "mHMLzWzSmTiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "ddf_clean = ddf.map_partitions(clean_and_label)\n",
        "ddf_tokenized = ddf_clean.map_partitions(tokenize_and_filter)\n",
        "tfidf_dask = ddf_tokenized.map_partitions(\n",
        "    apply_tfidf,\n",
        "    300,\n",
        "    meta=pd.DataFrame(columns=[f\"feature_{i}\" for i in range(300)] + [\"sentiment\"])\n",
        ")\n",
        "final_df = tfidf_dask.compute()\n",
        "end_time = time.time()\n",
        "print(f\"Total time taken: {end_time - start_time} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfwLef35lrDL",
        "outputId": "1d383274-3034-405d-f419-cb443a07fd5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total time taken: 95.95686388015747 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_dask = tfidf_dask.persist()\n",
        "train_ddf, test_ddf = train_test_split(\n",
        "    tfidf_dask, test_size=0.2, random_state=42, shuffle=True\n",
        ")\n",
        "train_ddf, test_ddf = train_ddf.persist(), test_ddf.persist()"
      ],
      "metadata": {
        "id": "47okK7RBq97U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_column = \"sentiment\"\n",
        "feature_columns = [col for col in tfidf_dask.columns if col != target_column]\n",
        "\n",
        "X_train = train_ddf[feature_columns].to_dask_array(lengths=True)\n",
        "y_train = train_ddf[target_column].to_dask_array(lengths=True)\n",
        "X_test = test_ddf[feature_columns].to_dask_array(lengths=True)\n",
        "y_test = test_ddf[target_column].to_dask_array(lengths=True)"
      ],
      "metadata": {
        "id": "ZZ0KrIaKqyhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lgb_params = {\n",
        "    \"objective\": \"binary\",\n",
        "    \"metric\": [\"auc\"],\n",
        "    \"boosting_type\": \"gbdt\",\n",
        "    \"num_leaves\": 31,\n",
        "    \"learning_rate\": 0.1,\n",
        "    \"random_state\": 42,\n",
        "}\n",
        "\n",
        "start_time = time.time()\n",
        "dask_model = DaskLGBMClassifier(**lgb_params)\n",
        "dask_model.fit(\n",
        "    X_train, y_train,\n",
        "    eval_set=[(X_test, y_test)],\n",
        "    feature_name=feature_columns\n",
        ")\n",
        "end_time = time.time()\n",
        "print(f\"Total time taken: {end_time - start_time} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYshQNH8ssP9",
        "outputId": "ea6673cb-87bf-4e1e-ff4b-144f72be4a8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/lightgbm/dask.py:549: UserWarning: Parameter n_jobs will be ignored.\n",
            "  _log_warning(f\"Parameter {param_alias} will be ignored.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finding random open ports for workers\n",
            "Total time taken: 15.492602825164795 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = dask_model.feature_name_\n",
        "print(feature_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyNiQyty1f66",
        "outputId": "53307e45-81bc-4426-819a-ce45de334618"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4', 'feature_5', 'feature_6', 'feature_7', 'feature_8', 'feature_9', 'feature_10', 'feature_11', 'feature_12', 'feature_13', 'feature_14', 'feature_15', 'feature_16', 'feature_17', 'feature_18', 'feature_19', 'feature_20', 'feature_21', 'feature_22', 'feature_23', 'feature_24', 'feature_25', 'feature_26', 'feature_27', 'feature_28', 'feature_29', 'feature_30', 'feature_31', 'feature_32', 'feature_33', 'feature_34', 'feature_35', 'feature_36', 'feature_37', 'feature_38', 'feature_39', 'feature_40', 'feature_41', 'feature_42', 'feature_43', 'feature_44', 'feature_45', 'feature_46', 'feature_47', 'feature_48', 'feature_49', 'feature_50', 'feature_51', 'feature_52', 'feature_53', 'feature_54', 'feature_55', 'feature_56', 'feature_57', 'feature_58', 'feature_59', 'feature_60', 'feature_61', 'feature_62', 'feature_63', 'feature_64', 'feature_65', 'feature_66', 'feature_67', 'feature_68', 'feature_69', 'feature_70', 'feature_71', 'feature_72', 'feature_73', 'feature_74', 'feature_75', 'feature_76', 'feature_77', 'feature_78', 'feature_79', 'feature_80', 'feature_81', 'feature_82', 'feature_83', 'feature_84', 'feature_85', 'feature_86', 'feature_87', 'feature_88', 'feature_89', 'feature_90', 'feature_91', 'feature_92', 'feature_93', 'feature_94', 'feature_95', 'feature_96', 'feature_97', 'feature_98', 'feature_99', 'feature_100', 'feature_101', 'feature_102', 'feature_103', 'feature_104', 'feature_105', 'feature_106', 'feature_107', 'feature_108', 'feature_109', 'feature_110', 'feature_111', 'feature_112', 'feature_113', 'feature_114', 'feature_115', 'feature_116', 'feature_117', 'feature_118', 'feature_119', 'feature_120', 'feature_121', 'feature_122', 'feature_123', 'feature_124', 'feature_125', 'feature_126', 'feature_127', 'feature_128', 'feature_129', 'feature_130', 'feature_131', 'feature_132', 'feature_133', 'feature_134', 'feature_135', 'feature_136', 'feature_137', 'feature_138', 'feature_139', 'feature_140', 'feature_141', 'feature_142', 'feature_143', 'feature_144', 'feature_145', 'feature_146', 'feature_147', 'feature_148', 'feature_149', 'feature_150', 'feature_151', 'feature_152', 'feature_153', 'feature_154', 'feature_155', 'feature_156', 'feature_157', 'feature_158', 'feature_159', 'feature_160', 'feature_161', 'feature_162', 'feature_163', 'feature_164', 'feature_165', 'feature_166', 'feature_167', 'feature_168', 'feature_169', 'feature_170', 'feature_171', 'feature_172', 'feature_173', 'feature_174', 'feature_175', 'feature_176', 'feature_177', 'feature_178', 'feature_179', 'feature_180', 'feature_181', 'feature_182', 'feature_183', 'feature_184', 'feature_185', 'feature_186', 'feature_187', 'feature_188', 'feature_189', 'feature_190', 'feature_191', 'feature_192', 'feature_193', 'feature_194', 'feature_195', 'feature_196', 'feature_197', 'feature_198', 'feature_199', 'feature_200', 'feature_201', 'feature_202', 'feature_203', 'feature_204', 'feature_205', 'feature_206', 'feature_207', 'feature_208', 'feature_209', 'feature_210', 'feature_211', 'feature_212', 'feature_213', 'feature_214', 'feature_215', 'feature_216', 'feature_217', 'feature_218', 'feature_219', 'feature_220', 'feature_221', 'feature_222', 'feature_223', 'feature_224', 'feature_225', 'feature_226', 'feature_227', 'feature_228', 'feature_229', 'feature_230', 'feature_231', 'feature_232', 'feature_233', 'feature_234', 'feature_235', 'feature_236', 'feature_237', 'feature_238', 'feature_239', 'feature_240', 'feature_241', 'feature_242', 'feature_243', 'feature_244', 'feature_245', 'feature_246', 'feature_247', 'feature_248', 'feature_249', 'feature_250', 'feature_251', 'feature_252', 'feature_253', 'feature_254', 'feature_255', 'feature_256', 'feature_257', 'feature_258', 'feature_259', 'feature_260', 'feature_261', 'feature_262', 'feature_263', 'feature_264', 'feature_265', 'feature_266', 'feature_267', 'feature_268', 'feature_269', 'feature_270', 'feature_271', 'feature_272', 'feature_273', 'feature_274', 'feature_275', 'feature_276', 'feature_277', 'feature_278', 'feature_279', 'feature_280', 'feature_281', 'feature_282', 'feature_283', 'feature_284', 'feature_285', 'feature_286', 'feature_287', 'feature_288', 'feature_289', 'feature_290', 'feature_291', 'feature_292', 'feature_293', 'feature_294', 'feature_295', 'feature_296', 'feature_297', 'feature_298', 'feature_299']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_probs = dask_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "auc_score = roc_auc_score(y_test.compute(), pred_probs.compute())\n",
        "print(f\"Validation AUC for Dask LightGBM: {auc_score:.4f}\")\n",
        "\n",
        "preds = dask_model.predict(X_test)\n",
        "accuracy = (preds == y_test).mean().compute()\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "5IdYiVTVhwY-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}